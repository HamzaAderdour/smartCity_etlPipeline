# Pipeline ETL pour l'analyse de donnÃ©es de capteurs IoT

## Description
Ce projet implÃ©mente un pipeline ETL complet pour l'analyse en temps rÃ©el des donnÃ©es de capteurs IoT dans une ville intelligente. Le systÃ¨me collecte, traite et analyse les donnÃ©es de tempÃ©rature, d'humiditÃ© et de pollution provenant de capteurs rÃ©partis dans diffÃ©rents quartiers.

## Architecture Technique

### 1. Data Ingestion Layer
- **Kafka Producers** : Collecte de donnÃ©es depuis APIs rÃ©elles
- **Topics Kafka** :
  - `raw-sensor-data` : donnÃ©es brutes des capteurs
  - `processed-data` : donnÃ©es traitÃ©es
  - `alerts` : alertes et anomalies
- **Sources de donnÃ©es** :
  - APIs mÃ©tÃ©o (OpenWeatherMap)
  - APIs pollution (OpenAQ)
  - Capteurs simulÃ©s pour complÃ©ter

### 2. Processing Layer
- **Airflow DAGs** :
  - Pipeline ETL toutes les 10 minutes
  - Analyses historiques quotidiennes/hebdomadaires
- **Celery Workers** :
  - Nettoyage des donnÃ©es (suppression outliers)
  - Calculs d'agrÃ©gations (moyennes horaires/quotidiennes)
  - DÃ©tection d'anomalies

### 3. Storage Layer
- **Elasticsearch** :
  - Index `raw-sensor-data` : donnÃ©es brutes
  - Index `hourly-aggregations` : moyennes horaires
  - Index `daily-aggregations` : moyennes quotidiennes
  - Index `alerts` : anomalies dÃ©tectÃ©es

### 4. API Layer
- **FastAPI Endpoints** :
  - `/api/v1/sensors/real-time` : donnÃ©es en temps rÃ©el
  - `/api/v1/aggregations/hourly/{district}` : moyennes horaires
  - `/api/v1/historical/{sensor_type}` : donnÃ©es historiques
  - `/api/v1/alerts` : anomalies dÃ©tectÃ©es
  - `/api/v1/export/{format}` : export CSV/Excel

### 5. Frontend Layer
- Dashboard React responsive (microservice sÃ©parÃ©)

## Configuration des Services
Services Docker Compose :
- zookeeper
- kafka
- elasticsearch
- airflow-webserver
- airflow-scheduler
- celery-worker
- redis (broker Celery)
- fastapi

## Types de DonnÃ©es CollectÃ©es
### Capteurs Environnementaux
- **TempÃ©rature** : Â°C (outliers: < -50Â°C ou > 60Â°C)
- **HumiditÃ©** : % (outliers: < 0% ou > 100%)
- **Pollution** :
  - PM2.5, PM10 (Âµg/mÂ³)
  - NO2, SO2, CO (ppb)
  - Indice AQI

## Workflow ETL
### 1. Extract (Toutes les 10 minutes)
- Collecte depuis APIs externes
- Insertion dans Kafka topics
- Validation des formats

### 2. Transform
- Nettoyage (suppression outliers)
- Enrichissement (ajout mÃ©tadonnÃ©es)
- Calculs d'agrÃ©gations
- DÃ©tection d'anomalies

### 3. Load
- Stockage dans Elasticsearch (donnÃ©es aberrantes et clean sÃ©parÃ©es)
- Indexation optimisÃ©e pour requÃªtes
- Archivage des donnÃ©es anciennes

## APIs Externes
### DonnÃ©es MÃ©tÃ©o
- OpenWeatherMap : TempÃ©rature, humiditÃ©, pression
### DonnÃ©es Pollution
- OpenAQ : QualitÃ© de l'air mondial

## FonctionnalitÃ©s
âœ… Collecte en continu depuis APIs rÃ©elles
âœ… Pipeline ETL orchestrÃ© par Airflow
âœ… Traitement asynchrone avec Celery
âœ… Stockage optimisÃ© avec Elasticsearch
âœ… API complÃ¨te avec FastAPI
âœ… Dashboard React responsive
âœ… PossibilitÃ© d'export des donnÃ©es
âœ… Analyses historiques avancÃ©es

## Structure du projet
```
â”œâ”€â”€ ğŸ“‚ config/           # Fichiers de configuration
â”œâ”€â”€ ğŸ“‚ data-ingestion/   # Composants d'ingestion Kafka
â”œâ”€â”€ ğŸ“‚ etl-pipeline/     # Pipeline ETL (Airflow, Celery)
â”œâ”€â”€ ğŸ“‚ data-storage/     # Stockage Elasticsearch
â”œâ”€â”€ ğŸ“‚ api/             # API FastAPI
```

## Installation
1. Cloner le repository
2. Installer les dÃ©pendances : `pip install -r requirements.txt`
3. Configurer les variables d'environnement
4. Lancer les services (instructions dÃ©taillÃ©es Ã  venir)

## Configuration
Les fichiers de configuration se trouvent dans le dossier `config/` :
- `airflow.cfg` : Configuration Airflow
- `kafka.yml` : Configuration Kafka
- `elasticsearch.yml` : Configuration Elasticsearch
- `env.yml` : Variables d'environnement

## DÃ©veloppement
Instructions pour le dÃ©veloppement Ã  venir...

## Licence
PropriÃ©taire - Tous droits rÃ©servÃ©s 